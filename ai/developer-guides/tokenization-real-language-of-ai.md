# Tokenization: The Real Language of AI

## Overview
Tokenization is the process of converting text into smaller units called tokens that a language model can process.

## Why Tokenization Exists
Neural networks operate on numbers, not text.
Tokenization bridges human language and numerical computation.

## Types of Tokens
- Words
- Subwords
- Characters

## Token IDs and Embeddings
Tokens are mapped to numerical IDs and then converted into vector representations.

## Implications for Prompting
Token limits affect context size, cost, and model performance.

## Summary
Tokenization defines how models read and generate language.
